# Feature Engineering with PySpark
## John Hogue

# Exploratory Data Analysis
- **Feature Engineering** is using domain knowledge to help our models perform better.
- While this class will be useful, don't just copy/paste.
- Before modeling, spend time defining your goals.
- Make sure to research your data and understand its limitations.
- Still be willing to ask interesting or challenging questions.
- The **Data Science Process** is:
  * Project Scoping/Data Collection.
  * Exploratory Analysis.
  * Data Cleaning.
  * Feature Engineering.
  * Model Training.
  * Project Delivery/Insights.
- This is a loop and not really ordered.
- As an aside, make sure to keep up with Spark's Documentation.
- You can check your version of spark using:
```python
spark.version

import sys
sys.version_info
```
- We'll be using a *Parquet File* for the data.
- This format is columnar; meaning organized by columns.
- They're also defined and typed.
- There are many different input file types:
```python
# JSON
spark.read.json('example.json')
 # CSV or delimited files
spark.read.csv('example.csv')
 # Parquet
spark.read.parquet('example.parq')
 # Read a parquet file to a PySpark DataFrame
df = spark.read.parquet('example.parq')
```
- You can print the columns of a dataframe in spark like in pandas:
```python
# Print columns in dataframe
print(df.columns)
```
- The first step is to *formally define your problem*.
- Next, we need to take into consideration the limitations of the data we have.
- Then, we check over our given attributes.
- You can get the number of rows using the function `df.count()`.
- You can take the length of the columns using `len( df.columns )`.
- Make sure that the data types are what you expect them to be.
- You can check using `df.dtypes` to get a list of tuples of (Name, type).
- You can display a column's summary statistics using:
```python
# Display summary statistics
Y_df.describe().show()
```
- In the field, you will be dealing with "less than ideal" data.
- Since the function `mean()` is considered an aggregate function, it will need to be passed with the function `df.agg()`.
```python
df.agg({'SALESCLOSEPRICE': 'mean'}).collect()
```
![Sales Close Price Generated by Seaborn in Python](images/salesmean.png)
- You use the function `.collect()` to force it to return a result now.
- The function `cov()` allows us to check the covariance of two attributes.
- You should explore your data with plots and the library **Seaborn** is the default.
- To do this though, you will need to convert the PySpark Dataframe to a Pandas dataframe.
- You can convert a Pyspark dataframe to pandas using `.toPandas()`
- You can use the function `sample()` to assist with getting a representative sample of data.
```python
# Sample 50% of the PySpark DataFrame and count rows; replacement off
df.sample(False, 0.5, 42).count()
```
```
# Import your favorite visualization library
import seaborn as sns
# Sample the dataframe
sample_df = df.select(['SALESCLOSEPRICE']).sample(False, 0.5, 42)
# Convert the sample to a Pandas DataFrame
pandas_df = sample_df.toPandas()
# Plot it
sns.distplot(pandas_df)
```
```
# Check the correlation of a pair of columns
corr_val = df.corr('SALESCLOSEPRICE', col)
```
```
# Import skewness function
from pyspark.sql.functions import skewness

# Compute and print skewness of LISTPRICE
print(df.agg({'LISTPRICE': 'skewness'}).collect())
```

# Wrangling with Spark Functions
- Garbage in, Garbage Out Mantra.
- Issues that can crop up with data:
  * Recorded wrong.
  * Unique events.
  * Formatted Incorrectly.
  * Duplications.
  * Missing.
  * Data not relevant.
- You can drop columns using the function `df.drop()`
```python
# List of columns to drop
cols_to_drop = ['NO', 'UNITNUMBER', 'CLASS']
# Drop the columns; don't forget the star.
df = df.drop(*cols_to_drop)
```
- You can filter text in a dataframe using:
  1. `.where()` by passing a condition.
  2. `.like()` is similar to SQL like for pattern matching.
  3. The `~` means logical not inside these conditions.
```
df = df.where(~df['POTENTIALSHORTSALE'].like('Not Disclosed'))
```
- It might be worth filtering Outliers.
- You can drop `NA` or `Null` values using the function `df.dropna()`
  * `.dropna()` by itself will drop any record that has any.
  * `.dropna( how=all, subset['col1', 'col2'])` will drop columns only if all of `col1` and `col2` have NAs.
  * `.dropna( thresh = 2)` will drop the record if `<n>` values are found.
- You can drop duplicate rows with the function `df.dropDuplicates()`.
- You can always pass a list of column names to limit the drop search to only certain columns.
```python
# Filter the text values out of df but keep null values
text_filter = ~df['ASSUMABLEMORTGAGE'].isin(yes_values) | df['ASSUMABLEMORTGAGE'].isNull()
df = df.where(text_filter)
```
- "Data does not give up its secrets easily; it must confess" *Jeff Huber*
- One way to deal with data is to scale it so that it's on the same scale.
- This is sometimes called **MinMax Scalaing**
![MinMax Scaling formula]()
- **Standardization** is when you transform the data into a Standard Normal Distribution.
- I way to manage skewed data is to log transform it.
```
# import the log function
from pyspark.sql.functions import log
 # Recalculate log of SALESCLOSEPRICE
df = df.withColumn('log_SalesClosePrice', log(df['SALESCLOSEPRICE']))
```
```
# Compute the skewness
print(df.agg({"YEARBUILT": 'skewness'}).collect())
```
- There are a few kinds of different missing values:
  * **Missing Completely at Random**: just completely random.
  * **Missing at Random**: Missing conditionally at random based on another observation.
  * **Missing Not at Random**: Data is missing because of how it is collected.
- If you only have a few missing values and they're missing *completely at random* then it might be fine to remove the rows.
- You can also use visualizations to assist with reasoning about missing values:
```python
# Import library
import seaborn as sns
# subset the dataframe
sub_df = df.select(['ROOMAREA1'])
# sample the dataframe
sample_df = sub_df.sample(False, .5, 4)
# Convert to Pandas DataFrame
pandas_df = sample_df.toPandas()
# Plot it
sns.heatmap(data=pandas_df.isnull())
```
- Another way to deal with missing data is to do an Imputation of Missing Values.
- Make sure you **really** understand the data before even attempting to make decisions about imputing missing values.
- You can use the function `df.fillna()` to replace missing values:
```python
# Replacing missing values with zero
df.fillna(0, subset=['DAYSONMARKET'])

# Replacing with the mean value for that column
col_mean = df.agg({'DAYSONMARKET': 'mean'}).collect()[0][0]
df.fillna(col_mean, subset=['DAYSONMARKET'])
```
```python
# Sample the dataframe and convert to Pandas
sample_df = df.select( columns ).sample(False, 0.5, 42)
pandas_df = sample_df.toPandas()

# Convert all values to T/F
tf_df = pandas_df.isnull()

# Plot it
sns.heatmap(data=tf_df)
plt.xticks(rotation=30, fontsize=10)
plt.yticks(rotation=0, fontsize=10)
plt.show()
```
- Adding external data may add important predictors, but too many features might end up with spurious correlations.
- The extra data may supplement or replace values, but could also induce data leakage.
- Getting more data could be really cheap to include, but just means that you'll need domain expertise to understand it.
- There are a few ways to do joins, but will mostly be *Left* and *Inner* Joins.
- An example:
```python
DataFrame.join(
    other,      # Other DataFrame to merge
    on=None,    # The keys to join on
    how=None)   # Type of join to perform (default is 'inner')
```
- If you're more familiar with SQL statements, then *PySpark* provides the function `spark.sql()`:
```python
# Register the dataframe as a temp table
df.createOrReplaceTempView("df")
hdf.createOrReplaceTempView("hdf")

# Write a SQL Statement
sql_df = spark.sql("""
                      SELECT
                        *
                      FROM df
                      LEFT JOIN hdf
                      ON df.OFFMARKETDATE = hdf.dt
                   """)
```
```python
# Cast data types
walk_df = walk_df.withColumn('longitude', walk_df['longitude'].cast('double'))
walk_df = walk_df.withColumn('latitude', walk_df['latitude'].cast('double'))

# Round percision
df = df.withColumn('longitude', round(df['longitude'], 5))
df = df.withColumn('latitude', round(df['latitude'], 5))

# Create join condition
condition = [(df['longitude'] == walk_df['longitude']), (df['latitude'] == walk_df['latitude'])]

# Join the dataframes together
join_df = df.join(walk_df, on=condition, how='left')
# Count non-null records from new field
print(join_df.where(~join_df['walkscore'].isNull()).count())
```
```python
# Register dataframes as tables
df.createOrReplaceTempView("df")
walk_df.createOrReplaceTempView("walk_df")

# SQL to join dataframes
join_sql = 	"""
			SELECT
				*
			FROM df
			LEFT JOIN walk_df
			ON df.longitude = walk_df.longitude
			AND df.latitude = walk_df.latitude
			"""
# Perform sql join
joined_df = spark.sql(join_sql)
```


# Feature Engineering
- Simply because you have data and Machine Learning does not mean it can figure out everything itself.
- You can generate new features using Multiplication, Summing, Differencing and Dividing.
```python
# Linear model plots
sns.jointplot(x="Total_SQFT", y="SALESCLOSEPRICE", data=pandas_df, kind="reg", stat_func=r2)
plt.show()
sns.jointplot(x="BATHS_PER_1000SQFT", y="SALESCLOSEPRICE", data=pandas_df, kind="reg", stat_func=r2)
plt.show()
```
- We want to assist our model in managing cycles in data.
- You can convert to a data using the function `to_date()`.
- If you want to keep the timestamps, then use function `to_timestamp()`.
- You can also extract the year and month once in time format using the functions `year()` and `month()`.
- You can also get the day of the month or the week of the year with functions `dayofmonth()` and `yearofmonth()`.
- **Lagging Features** is for features where there is a time difference between when the impact is felt and the event.
- We will need the function `window()` to allow us to run an aggregation against a group of records.
- The function `lag()` will return the value that is `count` rows before the current row.
```python
from pyspark.sql.functions import lag
from pyspark.sql.window import Window
 # Create Window
w = Window().orderBy(m_df['DATE'])
 # Create lagged column
m_df = m_df.withColumn('MORTGAGE-1wk', lag('MORTGAGE', count=1).over(w))
 # Inspect results
m_df.show(3)
```
- You can calculate the difference between two dates using the function `datediff()`.
```python
# Calculate difference between date columns
mort_df = mort_df.withColumn('Days_Between_Report', datediff('DATE', 'DATE-1'))
# Print results
mort_df.select('Days_Between_Report').distinct().show()
```
- Extracting and creating a Boolean value form text can be done with the function `when()`
```python
# Create boolean filters
find_under_8 = df['ROOF'].like('%Age 8 Years or Less%')
find_over_8 = df['ROOF'].like('%Age Over 8 Years%')

# Apply filters using when() and otherwise()
df = df.withColumn('old_roof', (when(find_over_8, 1)
                               .when(find_under_8, 0)
                               .otherwise(None)))
```
- You can split a column using the function `split()`.
```python
# Put the first value of the list into a new column
df = df.withColumn('Roof_Material', split_col.getItem(0))
# Inspect results
df[['ROOF', 'Roof_Material']].show(5, truncate=100)
```
- **Exploding** is when you take the values from a split and duplicate the columns.

```python
from pyspark.sql.functions import split, explode, lit, coalesce, first

 # Split the column on commas into a list
df = df.withColumn('roof_list', split(df['ROOF'], ', '))

 # Explode list into new records for each value
ex_df = df.withColumn('ex_roof_list', explode(df['roof_list']))

 # Create a dummy column of constant value
ex_df = ex_df.withColumn('constant_val', lit(1))

 # Pivot the values into boolean columns
piv_df = ex_df.groupBy('NO').pivot('ex_roof_list')\
  .agg(coalesce(first('constant_val')))
```

- **Binarizing** is a great way to collapse some of the complexity of the features to YES or NO.

```python
from pyspark.ml.feature import Binarizer
 # Cast the data type to double
df = df.withColumn('FIREPLACES', df['FIREPLACES'].cast('double'))
 # Create binarizing transformer
bin = Binarizer(threshold=0.0, inputCol='FIREPLACES', outputCol='FireplaceT')
# Apply the transformer
df = bin.transform(df)
```

- **Bucketing** is a way to define ordinal values.

```python
from pyspark.ml.feature import Bucketizer
# Define how to split data
splits = [0, 1, 2, 3, 4, float('Inf')]
# Create bucketing transformer
buck = Bucketizer(splits=splits, inputCol='BATHSTOTAL', outputCol='baths')
# Apply transformer
df = buck.transform(df)
# Inspect results
df[['BATHSTOTAL', 'baths']].show(4)
```

- **One Hot Encoding** is where you pivot values in a column into their own TRUE/FALSE columns.

```python
from pyspark.ml.feature import OneHotEncoder, StringIndexer
# Create indexer transformer
stringIndexer = StringIndexer(inputCol='CITY', outputCol='City_Index')
 # Fit transformer
model = stringIndexer.fit(df)
# Apply transformer
indexed = model.transform(df)

# Create encoder transformer
encoder = OneHotEncoder(inputCol='City_Index', outputCol='City_Vec)

# Apply the encoder transformer
encoded_df = encoder.transform(indexed)

# Inspect results
encoded_df[['City_Vec']].show(4)
```

- Note that the last value in the list is not included since it is linearly dependent on the others.
-


python# Building a Model

# Research:
- FeatureTools package.
- TSFresh package.


# Reference:
