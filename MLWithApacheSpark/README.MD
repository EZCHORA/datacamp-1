# Machine Learning With Apache Spark
## Andrew Collier

# Introduction
- There are two ways ways to approach giving computers:
  1. Give them explicit instructions to follow.
  2. Give them lots of examples and have it write the rules.
- Most Machine Learning problems into Regression and Classification.
- The performance of these models is dependent on Data.
- In general, more data is usually better.
- However, as we get more data we end up having to wait longer due to IO and calculations.
- One solution for this is distributed across a cluster.
- Spark is a general purpose framework for cluster computing.
- Anatomy of a Spark Cluster:
  * One of more **Nodes**.
  * **Node**: is a computer with RAM, CPU and storage.
  * A **Cluster Manager** allocates resources and coordinates activity across the cluster.
  * A **Driver** communicates with the *Cluster Manager* who then allocates out the work.
  * On each *Node*, Spark launches **Executors** which manages tasks.
  * 


# Classification

# Regression

# Ensembles and Pipelines

# Reference:

# Research:
